{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Model Training\n\n## 1. Setting Up Spark Context"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession",
            "execution_count": 1,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()",
            "execution_count": 2,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## 2. Download data from Object Store"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import os\nimport getpass\n\ndef get_or_set_environment_variable(variable):\n    try:\n        var = os.environ[variable]\n    except KeyError:\n        var = getpass.getpass('Please enter value for {:}: '.format(variable))\n    \n    os.environ[variable] = var\n    return var\n\nibm_api_key_id = get_or_set_environment_variable('IBM_API_KEY_ID')\nibm_cloud_store_bucket = get_or_set_environment_variable('IBM_OBJECT_STORE_BUCKET')",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Please enter value for IBM_API_KEY_ID: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nPlease enter value for IBM_OBJECT_STORE_BUCKET: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import json\nimport os\n\nimport types\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\nclient = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=ibm_api_key_id,\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client.get_object(Bucket=ibm_cloud_store_bucket,\n                         Key='feature_eng_parquet_files.json')['Body']\n# add missing __iter__ method\n\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\nfiles = json.load(body)\nfiles",
            "execution_count": 4,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 4,
                    "data": {
                        "text/plain": "{'train': ['disaster_detection_clean_train-0000.parquet'],\n 'test': ['disaster_detection_clean_test-0000.parquet'],\n 'label': ['disaster_detection_label-0000.parquet']}"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def load_dataframe(files, **kargs):\n    dfs = []\n    for fn in files:\n        body = client.get_object(Bucket=ibm_cloud_store_bucket,\n                                 Key=fn)['Body']\n        if not hasattr(body, \"__iter__\"):\n            body.__iter__ = types.MethodType( __iter__, body )\n        \n        tfn = 'temp_{:}'.format(fn)\n        with open(tfn, 'wb') as temp:\n            temp.write(body.read())\n        dfs.append(spark.read.options(**kargs).parquet(tfn))\n    df = dfs.pop()\n    for other in dfs:\n        df = df.union(other)\n    return df\n\ndf_train = load_dataframe(files['train'])",
            "execution_count": 5,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df_train.first()",
            "execution_count": 6,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 6,
                    "data": {
                        "text/plain": "Row(id=1, text='Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all', features_count=SparseVector(2266, {16: 1.0, 80: 1.0, 201: 1.0, 451: 1.0, 1499: 1.0, 1917: 1.0}), features_tfidf=SparseVector(2500, {26: 3.5396, 166: 4.4269, 336: 4.9488, 689: 6.8583, 768: 4.7946, 1830: 6.5398, 2174: 5.942}), features_w2v=DenseVector([-0.0027, -0.0033, -0.0001, -0.0146, -0.0046, -0.0019, 0.0145, -0.0033, 0.0021, -0.0035, 0.0034, -0.0032, 0.0055, 0.0025, 0.0017, -0.0042, -0.0064, 0.0136, -0.0145, 0.0039, -0.0013, -0.0159, 0.0002, -0.0047, -0.0079, 0.0002, 0.003, 0.0003, 0.0128, 0.0077, 0.0042, 0.0006, -0.0179, 0.0065, 0.0165, 0.0014, 0.0083, 0.0085, -0.0005, -0.0023, -0.0029, 0.0114, -0.0027, -0.006, -0.0053, -0.0076, -0.007, -0.0069, 0.0033, 0.0037, -0.0085, -0.0051, -0.0048, 0.002, -0.0105, 0.0108, 0.0022, 0.0136, -0.0087, 0.0014, -0.0045, -0.0036, -0.0108, 0.0035, -0.0076, -0.0008, 0.004, -0.0032, 0.0056, -0.0102, -0.0083, 0.0029, 0.0036, -0.008, -0.003, -0.0052, 0.0015, -0.0037, 0.0045, 0.0091, -0.0042, -0.0082, 0.0024, 0.0078, -0.0054, -0.0019, -0.0083, -0.0127, 0.0021, -0.0121, 0.0036, -0.0036, -0.0017, 0.0012, 0.0139, 0.0034, -0.0038, 0.0039, -0.0066, 0.0001]))"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## 2. Model definition\n\nWe try 3 different models\n\n* logistic regression,\n* multinomial naive Bayes, and\n* a convolutional neural network"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 2.1 Logistic Regression"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.classification import LogisticRegression\n\nlrs = [LogisticRegression(featuresCol=feat,\n                          maxIter=20, regParam=0.3, elasticNetParam=0)\n       for feat in ('features_count', 'features_tfidf')]\nlrs",
            "execution_count": 7,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 7,
                    "data": {
                        "text/plain": "[LogisticRegression_c8b0de18d229, LogisticRegression_20d068424298]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 2.2 Naive Bayes"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.classification import NaiveBayes\n\nnbs = [NaiveBayes(featuresCol=feat, smoothing=1)\n       for feat in ('features_count', 'features_tfidf')]\nnbs",
            "execution_count": 8,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 8,
                    "data": {
                        "text/plain": "[NaiveBayes_eadba888dbea, NaiveBayes_9056b7fbe3e8]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### 2.3 Convolutional Neural Network"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import tensorflow as tf\ntf.__version__",
            "execution_count": 9,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 9,
                    "data": {
                        "text/plain": "'2.2.0-rc0'"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "from tensorflow.keras.layers import Dense, Dropout, LeakyReLU\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import optimizers\n\nMAX_SEQUENCE_LENGTH = len(df_train.select('features_w2v').first()['features_w2v'])\n\nmodel = Sequential()\nmodel.add(Dense(82, input_dim=MAX_SEQUENCE_LENGTH))\nmodel.add(LeakyReLU(alpha=0.01))\nmodel.add(Dropout(0.20))\nmodel.add(Dense(82))\nmodel.add(LeakyReLU(alpha=0.01))\nmodel.add(Dropout(0.20))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])",
            "execution_count": 10,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## 3. Serializing the Models"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!rm -rf *_*.ai",
            "execution_count": 11,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import os\nimport shutil\n\nspark_models = lrs + nbs\n\ndef serialize_spark_model(model, name, feature):\n    export_path = '{name:}_{feature:}.ai'.format(name=name, feature=feature)\n    model.save(export_path)\n    return shutil.make_archive(base_name=export_path,\n                               format='zip', base_dir=export_path)\n\nspark_paths = [serialize_spark_model(model, str(model).split('_')[0], feature)\n                for model, feature in zip(spark_models, ['count', 'tfidf'] * 2)]\nspark_paths",
            "execution_count": 12,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 12,
                    "data": {
                        "text/plain": "['LogisticRegression_count.ai.zip',\n 'LogisticRegression_tfidf.ai.zip',\n 'NaiveBayes_count.ai.zip',\n 'NaiveBayes_tfidf.ai.zip']"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def serialize_keras_model(model, name, feature):\n    export_path = '{name:}_{feature:}.ai.h5'.format(name=name, feature=feature)\n    model.save(export_path)\n    return export_path\n\nkeras_path = serialize_keras_model(model, 'Sequential_NN', 'w2v')\nkeras_path",
            "execution_count": 13,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 13,
                    "data": {
                        "text/plain": "'Sequential_NN_w2v.ai.h5'"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## 4. Uploading the files to object cloud"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def upload_model(client, path, model_key):\n    with open(path, 'rb') as modelF:\n        client.put_object(Bucket=ibm_cloud_store_bucket,\n                          Body=modelF,\n                          Key=model_key\n                         )\n    return model_key\n\nclient = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=ibm_api_key_id,\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nmodels = {upload_model(client, path, model_key=path): 'spark'\n          for path in spark_paths}\nmodels[upload_model(client, keras_path, model_key = keras_path)] = 'keras'\n\nmodels",
            "execution_count": 14,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 14,
                    "data": {
                        "text/plain": "{'LogisticRegression_count.ai.zip': 'spark',\n 'LogisticRegression_tfidf.ai.zip': 'spark',\n 'NaiveBayes_count.ai.zip': 'spark',\n 'NaiveBayes_tfidf.ai.zip': 'spark',\n 'Sequential_NN_w2v.ai.h5': 'keras'}"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import json\n\nclient.put_object(Bucket=ibm_cloud_store_bucket,\n                  Body=json.dumps(models),\n                  Key='model_def_files.json')",
            "execution_count": 15,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 15,
                    "data": {
                        "text/plain": "{'ResponseMetadata': {'RequestId': '07a41d92-bfb0-4b32-b6fd-4dc60fcaf2d4',\n  'HostId': '',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'date': 'Sat, 06 Feb 2021 21:48:13 GMT',\n   'x-clv-request-id': '07a41d92-bfb0-4b32-b6fd-4dc60fcaf2d4',\n   'server': 'Cleversafe',\n   'x-clv-s3-version': '2.5',\n   'x-amz-request-id': '07a41d92-bfb0-4b32-b6fd-4dc60fcaf2d4',\n   'etag': '\"6f57c0c7b085219d66ba44f66bbca793\"',\n   'content-length': '0'},\n  'RetryAttempts': 0},\n 'ETag': '\"6f57c0c7b085219d66ba44f66bbca793\"'}"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.7.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}