{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Model Training\n\n## 1. Setting Up Spark Context"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 2. Download data from Object Store"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Please enter value for IBM_API_KEY_ID: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nPlease enter value for IBM_OBJECT_STORE_BUCKET: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
                }
            ],
            "source": "import os\nimport getpass\n\ndef get_or_set_environment_variable(variable):\n    try:\n        var = os.environ[variable]\n    except KeyError:\n        var = getpass.getpass('Please enter value for {:}: '.format(variable))\n    \n    os.environ[variable] = var\n    return var\n\nibm_api_key_id = get_or_set_environment_variable('IBM_API_KEY_ID')\nibm_cloud_store_bucket = get_or_set_environment_variable('IBM_OBJECT_STORE_BUCKET')"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "{'train': ['desaster_detection_clean_train-0000.parquet'],\n 'test': ['desaster_detection_clean_test-0000.parquet'],\n 'label': ['desaster_detection_label-0000.parquet']}"
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "def load_dataframe(files, **kargs):\n    dfs = []\n    for fn in files:\n        body = client.get_object(Bucket=ibm_cloud_store_bucket,\n                                 Key=fn)['Body']\n        if not hasattr(body, \"__iter__\"):\n            body.__iter__ = types.MethodType( __iter__, body )\n        \n        tfn = 'temp_{:}'.format(fn)\n        with open(tfn, 'wb') as temp:\n            temp.write(body.read())\n        dfs.append(spark.read.options(**kargs).parquet(tfn))\n    df = dfs.pop()\n    for other in dfs:\n        df = df.union(other)\n    return df\n\ndf_train = load_dataframe(files['train'])"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "Row(id=1, text='Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all', features_count=SparseVector(2266, {16: 1.0, 80: 1.0, 201: 1.0, 451: 1.0, 1499: 1.0, 1917: 1.0}), features_tfidf=SparseVector(2500, {26: 3.5396, 166: 4.4269, 336: 4.9488, 689: 6.8583, 768: 4.7946, 1830: 6.5398, 2174: 5.942}), features_w2v=DenseVector([0.022, 0.0067, -0.0207, 0.0032, 0.0095, 0.0075, -0.0039, -0.0, 0.0158, -0.0062, -0.0029, -0.0127, -0.0061, 0.014, 0.0009, -0.0154, 0.0027, 0.0074, -0.0031, -0.0023, -0.0005, -0.0026, 0.0009, -0.0161, -0.0091, 0.0005, 0.0073, -0.0074, -0.0015, 0.0087, -0.0029, -0.0011, 0.0084, -0.0003, 0.0051, -0.013, 0.0004, 0.0016, 0.0048, 0.0002, -0.0008, -0.0099, 0.0017, -0.0099, -0.0031, -0.0015, 0.005, 0.0084, 0.0085, 0.0089, 0.0036, 0.0001, 0.0087, 0.0067, -0.004, 0.0007, -0.0074, 0.0003, 0.0063, -0.0049, 0.0033, -0.0159, 0.0056, 0.0039, 0.0032, 0.0061, -0.0081, 0.0047, 0.0053, -0.0037, 0.0182, -0.0019, -0.0083, -0.0153, 0.0087, -0.0044, -0.0165, -0.0275, -0.0041, -0.0082, 0.0018, 0.0027, 0.0112, -0.0014, 0.0078, 0.0003, 0.0001, -0.0106, 0.018, -0.008, 0.009, -0.0033, -0.0053, 0.0084, 0.005, -0.0086, 0.0066, -0.0019, 0.0227, 0.0006]))"
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df_train.first()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 2. Model definition\n\nWe try 3 different models\n\n* logistic regression,\n* multinomial naive Bayes, and\n* a convolutional neural network"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 2.1 Logistic Regression"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[LogisticRegression_6c76ceaefa53, LogisticRegression_05e1e1ba89f0]"
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from pyspark.ml.classification import LogisticRegression\n\nlrs = [LogisticRegression(featuresCol=feat,\n                          maxIter=20, regParam=0.3, elasticNetParam=0)\n       for feat in ('features_count', 'features_tfidf')]\nlrs"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 2.2 Naive Bayes"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[NaiveBayes_15f252f354da, NaiveBayes_f195de66bbed]"
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from pyspark.ml.classification import NaiveBayes\n\nnbs = [NaiveBayes(featuresCol=feat, smoothing=1)\n       for feat in ('features_count', 'features_tfidf')]\nnbs"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 2.3 Convolutional Neural Network"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "'2.2.0-rc0'"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "import tensorflow as tf\ntf.__version__"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "from tensorflow.keras.layers import Dense, Dropout, LeakyReLU\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import optimizers\n\nMAX_SEQUENCE_LENGTH = len(df_train.select('features_w2v').first()['features_w2v'])\n\nmodel = Sequential()\nmodel.add(Dense(82, input_dim=MAX_SEQUENCE_LENGTH))\nmodel.add(LeakyReLU(alpha=0.01))\nmodel.add(Dropout(0.20))\nmodel.add(Dense(82))\nmodel.add(LeakyReLU(alpha=0.01))\nmodel.add(Dropout(0.20))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 3. Serializing the Models"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": "!rm -rf *_*.ai"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "['LogisticRegression_count.ai.zip',\n 'LogisticRegression_tfidf.ai.zip',\n 'NaiveBayes_count.ai.zip',\n 'NaiveBayes_tfidf.ai.zip']"
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "import os\nimport shutil\n\nspark_models = lrs + nbs\n\ndef serialize_spark_model(model, name, feature):\n    export_path = '{name:}_{feature:}.ai'.format(name=name, feature=feature)\n    model.save(export_path)\n    return shutil.make_archive(base_name=export_path,\n                               format='zip', base_dir=export_path)\n\nspark_paths = [serialize_spark_model(model, str(model).split('_')[0], feature)\n                for model, feature in zip(spark_models, ['count', 'tfidf'] * 2)]\nspark_paths"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "'Sequential_NN_w2v.ai.h5'"
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "def serialize_keras_model(model, name, feature):\n    export_path = '{name:}_{feature:}.ai.h5'.format(name=name, feature=feature)\n    model.save(export_path)\n    return export_path\n\nkeras_path = serialize_keras_model(model, 'Sequential_NN', 'w2v')\nkeras_path"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 4. Uploading the files to object cloud"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "{'LogisticRegression_count.ai.zip': 'spark',\n 'LogisticRegression_tfidf.ai.zip': 'spark',\n 'NaiveBayes_count.ai.zip': 'spark',\n 'NaiveBayes_tfidf.ai.zip': 'spark',\n 'Sequential_NN_w2v.ai.h5': 'keras'}"
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "def upload_model(client, path, model_key):\n    with open(path, 'rb') as modelF:\n        client.put_object(Bucket=ibm_cloud_store_bucket,\n                          Body=modelF,\n                          Key=model_key\n                         )\n    return model_key\n\nclient = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=ibm_api_key_id,\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nmodels = {upload_model(client, path, model_key=path): 'spark'\n          for path in spark_paths}\nmodels[upload_model(client, keras_path, model_key = keras_path)] = 'keras'\n\nmodels"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "{'ResponseMetadata': {'RequestId': '711aaf2a-5abc-4dd3-b731-cfe436a12550',\n  'HostId': '',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'date': 'Wed, 27 Jan 2021 08:01:48 GMT',\n   'x-clv-request-id': '711aaf2a-5abc-4dd3-b731-cfe436a12550',\n   'server': 'Cleversafe',\n   'x-clv-s3-version': '2.5',\n   'x-amz-request-id': '711aaf2a-5abc-4dd3-b731-cfe436a12550',\n   'etag': '\"6f57c0c7b085219d66ba44f66bbca793\"',\n   'content-length': '0'},\n  'RetryAttempts': 0},\n 'ETag': '\"6f57c0c7b085219d66ba44f66bbca793\"'}"
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "import json\n\nclient.put_object(Bucket=ibm_cloud_store_bucket,\n                  Body=json.dumps(models),\n                  Key='model_def_files.json')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}