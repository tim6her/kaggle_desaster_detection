{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Data Cleansing and Feature Engineering\n\n## 1. Setting Up Spark Context"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession",
            "execution_count": 29,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()",
            "execution_count": 30,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## 2. Download data from Object Store"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import os\nimport getpass\n\ndef get_or_set_environment_variable(variable):\n    try:\n        var = os.environ[variable]\n    except KeyError:\n        var = getpass.getpass('Please enter value for {:}: '.format(variable))\n    \n    os.environ[variable] = var\n    return var\n\nibm_api_key_id = get_or_set_environment_variable('IBM_API_KEY_ID')\nibm_cloud_store_bucket = get_or_set_environment_variable('IBM_CLOUD_STORE_BUCKET')",
            "execution_count": 31,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import json\nimport os\n\nimport types\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=ibm_api_key_id,\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client.get_object(Bucket=ibm_cloud_store_bucket,\n                         Key='etl_parquet_files.json')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object \n\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\nfiles = json.load(body)\nfiles",
            "execution_count": 32,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 32,
                    "data": {
                        "text/plain": "{'train': ['desaster_detection_train-0000.parquet'],\n 'label': ['desaster_detection_label-0000.parquet'],\n 'test': ['desaster_detection_test-0000.parquet']}"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def load_dataframe(files):\n    dfs = []\n    for fn in files:\n        body = client.get_object(Bucket=ibm_cloud_store_bucket,\n                                 Key=fn)['Body']\n        if not hasattr(body, \"__iter__\"):\n            body.__iter__ = types.MethodType( __iter__, body )\n        \n        tfn = 'temp_{:}'.format(fn)\n        with open(tfn, 'wb') as temp:\n            temp.write(body.read())\n        dfs.append(spark.read.parquet(tfn))\n    df = dfs.pop()\n    for other in dfs:\n        df = df.union(other)\n    return df\n\ndf_train = load_dataframe(files['train'])\ndf_test = load_dataframe(files['test'])",
            "execution_count": 33,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df_train.schema == df_test.schema",
            "execution_count": 34,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 34,
                    "data": {
                        "text/plain": "True"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df_train.limit(10).toPandas()",
            "execution_count": 43,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 43,
                    "data": {
                        "text/plain": "   id                                               text\n0   1  Our Deeds are the Reason of this #earthquake M...\n1   4             Forest fire near La Ronge Sask. Canada\n2   5  All residents asked to 'shelter in place' are ...\n3   6  13,000 people receive #wildfires evacuation or...\n4   7  Just got sent this photo from Ruby #Alaska as ...\n5   8  #RockyFire Update => California Hwy. 20 closed...\n6  10  #flood #disaster Heavy rain causes flash flood...\n7  13  I'm on top of the hill and I can see a fire in...\n8  14  There's an emergency evacuation happening now ...\n9  15  I'm afraid that the tornado is coming to our a...",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8</td>\n      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>#flood #disaster Heavy rain causes flash flood...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>13</td>\n      <td>I'm on top of the hill and I can see a fire in...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>14</td>\n      <td>There's an emergency evacuation happening now ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>15</td>\n      <td>I'm afraid that the tornado is coming to our a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## 2. Data Cleansing"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def drop_unused_cols(df):\n    return df.drop('location', 'keyword')\n\ndf_train = drop_unused_cols(df_train)\ndf_test = drop_unused_cols(df_test)\n\ndf_train.limit(10).toPandas()",
            "execution_count": 44,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 44,
                    "data": {
                        "text/plain": "   id                                               text\n0   1  Our Deeds are the Reason of this #earthquake M...\n1   4             Forest fire near La Ronge Sask. Canada\n2   5  All residents asked to 'shelter in place' are ...\n3   6  13,000 people receive #wildfires evacuation or...\n4   7  Just got sent this photo from Ruby #Alaska as ...\n5   8  #RockyFire Update => California Hwy. 20 closed...\n6  10  #flood #disaster Heavy rain causes flash flood...\n7  13  I'm on top of the hill and I can see a fire in...\n8  14  There's an emergency evacuation happening now ...\n9  15  I'm afraid that the tornado is coming to our a...",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8</td>\n      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>#flood #disaster Heavy rain causes flash flood...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>13</td>\n      <td>I'm on top of the hill and I can see a fire in...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>14</td>\n      <td>There's an emergency evacuation happening now ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>15</td>\n      <td>I'm afraid that the tornado is coming to our a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## 4. Serializing the dataframes in *Parquet* format"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!rm -r ./desaster_detection_*",
            "execution_count": 37,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import glob\n\ntemp_parquet_file = os.path.join(os.path.curdir,\n                                 'desaster_detection_clean{}')\ndf_train.write.parquet(temp_parquet_file.format('train'), mode='overwrite')\ndf_test.write.parquet(temp_parquet_file.format('test'), mode='overwrite')\n\nglob.glob(temp_parquet_file.format('*'))",
            "execution_count": 40,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 40,
                    "data": {
                        "text/plain": "['./desaster_detection_cleantest', './desaster_detection_cleantrain']"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## 5. Uploading the files to object cloud"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def upload_parquet(client, path):\n    parts = glob.glob(os.path.join(path, '*.parquet'))\n    parquets = ['{:s}-{:04d}.parquet'.format(os.path.split(path)[-1], i)\n                for i in range(len(parts))]\n    for part, parquet in zip(parts, parquets):\n        with open(part, 'rb') as parquetF:\n            client.put_object(Bucket=ibm_cloud_store_bucket,\n                          Body=parquetF,\n                          Key=parquet\n                         )\n    return parquets\n\nclient = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=ibm_api_key_id,\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\n\nparquets = {}\nfor dataset in ('train', 'test'):\n    parquets[dataset] = upload_parquet(client, temp_parquet_file.format(dataset))\n\nprint(parquets)",
            "execution_count": 41,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "{'train': ['desaster_detection_cleantrain-0000.parquet'], 'test': ['desaster_detection_cleantest-0000.parquet']}\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import json\n\n\nparquets['label'] = files['label']\nclient.put_object(Bucket=ibm_cloud_store_bucket,\n                  Body=json.dumps(parquets),\n                  Key='feature_eng_parquet_files.json')",
            "execution_count": 42,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 42,
                    "data": {
                        "text/plain": "{'ResponseMetadata': {'RequestId': '0a300fde-e02a-424b-9c46-798cab505d66',\n  'HostId': '',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'date': 'Fri, 15 Jan 2021 16:09:20 GMT',\n   'x-clv-request-id': '0a300fde-e02a-424b-9c46-798cab505d66',\n   'server': 'Cleversafe',\n   'x-clv-s3-version': '2.5',\n   'x-amz-request-id': '0a300fde-e02a-424b-9c46-798cab505d66',\n   'etag': '\"32a7a4a0557df12cefd21b61af931764\"',\n   'content-length': '0'},\n  'RetryAttempts': 0},\n 'ETag': '\"32a7a4a0557df12cefd21b61af931764\"'}"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.7.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}