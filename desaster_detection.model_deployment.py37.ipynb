{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Model Evaluation\n\n## 1. Setting Up Spark Context"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 2. Download data from Object Store"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "import os\nimport getpass\n\ndef get_or_set_environment_variable(variable):\n    try:\n        var = os.environ[variable]\n    except KeyError:\n        var = getpass.getpass('Please enter value for {:}: '.format(variable))\n    \n    os.environ[variable] = var\n    return var\n\nibm_api_key_id = get_or_set_environment_variable('IBM_API_KEY_ID')\nibm_cloud_store_bucket = get_or_set_environment_variable('IBM_OBJECT_STORE_BUCKET')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 2.1 Loading Data for Evaluation"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "{'validation': ['desaster_detection_validation_validation-0000.parquet'],\n 'train': ['desaster_detection_clean_train-0000.parquet'],\n 'test': ['desaster_detection_clean_test-0000.parquet'],\n 'label': ['desaster_detection_label-0000.parquet']}"
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "def load_dataframe(files, **kargs):\n    dfs = []\n    for fn in files:\n        body = client.get_object(Bucket=ibm_cloud_store_bucket,\n                                 Key=fn)['Body']\n        if not hasattr(body, \"__iter__\"):\n            body.__iter__ = types.MethodType( __iter__, body )\n        \n        tfn = 'temp_{:}'.format(fn)\n        with open(tfn, 'wb') as temp:\n            temp.write(body.read())\n        dfs.append(spark.read.options(**kargs).parquet(tfn))\n    df = dfs.pop()\n    for other in dfs:\n        df = df.union(other)\n    return df\n\ndf_test = load_dataframe(files['test'])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 2.1 Loading Trained Models"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "{'LogisticRegressionModel_count_trained.ai.zip': 'spark',\n 'LogisticRegressionModel_tfidf_trained.ai.zip': 'spark',\n 'NaiveBayes_15f252f354da_count_trained.ai.zip': 'spark',\n 'NaiveBayes_f195de66bbed_tfidf_trained.ai.zip': 'spark',\n 'Sequential_NN_w2v_trained.ai.h5': 'keras'}"
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "client = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=ibm_api_key_id,\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client.get_object(Bucket=ibm_cloud_store_bucket,\n                         Key='model_train_files.json')['Body']\n\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\nmodel_files = json.load(body)\nmodel_files"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "'./temp_NaiveBayes_15f252f354da_count_trained.ai.zip'"
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "fn = list(model_files.keys())[2]\n\nbody = client.get_object(Bucket=ibm_cloud_store_bucket,\n                         Key=fn)['Body']\nif not hasattr(body, \"__iter__\"):\n    body.__iter__ = types.MethodType( __iter__, body )\n\ntfn = os.path.join(os.path.curdir, 'temp_{:}'.format(fn))\nwith open(tfn, 'wb') as temp:\n    temp.write(body.read())\n\ntfn"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "'./NaiveBayes_15f252f354da_count_trained.ai/'"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "import zipfile\n\ndef unzip_file(path):\n    with zipfile.ZipFile(path, 'r') as zip_ref:\n        zip_ref.extractall(os.curdir)\n        extracted = zip_ref.namelist()[0]\n    return os.path.join(os.curdir, extracted)\n\nextracted_model = unzip_file(tfn)\nextracted_model"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 2.2 Loading the Naive Bayes Model"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "NaiveBayes_15f252f354da"
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from pyspark.ml.classification import NaiveBayesModel\n\nmodel = NaiveBayesModel.load(extracted_model)\nmodel"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 3. Predicting the Test Data"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Just happened a terrible car crash</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>12</td>\n      <td>We're shaking...It's an earthquake</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>21</td>\n      <td>They'd probably still show more life than Arse...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>22</td>\n      <td>Hey! How are you?</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>27</td>\n      <td>What a nice hat?</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>29</td>\n      <td>Fuck off!</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   id                                               text  prediction\n0   0                 Just happened a terrible car crash         1.0\n1   2  Heard about #earthquake is different cities, s...         0.0\n2   3  there is a forest fire at spot pond, geese are...         1.0\n3   9           Apocalypse lighting. #Spokane #wildfires         1.0\n4  11      Typhoon Soudelor kills 28 in China and Taiwan         1.0\n5  12                 We're shaking...It's an earthquake         1.0\n6  21  They'd probably still show more life than Arse...         0.0\n7  22                                  Hey! How are you?         0.0\n8  27                                   What a nice hat?         0.0\n9  29                                          Fuck off!         0.0"
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "prediction = model.transform(df_test).select('id', 'text', 'prediction')\n\nprediction.limit(10).toPandas()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 4. Exporting the Data in Specified Format"
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>12</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>22</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>27</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>29</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   id  target\n0   0       1\n1   2       0\n2   3       1\n3   9       1\n4  11       1\n5  12       1\n6  21       0\n7  22       0\n8  27       0\n9  29       0"
                    },
                    "execution_count": 29,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from pyspark.sql.types import IntegerType\nimport pyspark.sql.functions as sfun\n\ndf_exp = prediction.select('id', sfun.col('prediction').cast(IntegerType()).alias('target'))\ndf_exp.limit(10).toPandas()"
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": "df_exp.toPandas().to_csv('submission.csv', index=False)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 5. Submitting the CSV to Kaggle"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": "import os\nimport json\n\nclient = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=ibm_api_key_id,\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client.get_object(Bucket=ibm_cloud_store_bucket,\n                         Key='kaggle.json')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object \n\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ncreds = json.load(body)\nos.environ['KAGGLE_USERNAME'] = creds['username']\nos.environ['KAGGLE_KEY'] = creds['key']"
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 22.2k/22.2k [00:00<00:00, 34.8kB/s]\nSuccessfully submitted to Natural Language Processing with Disaster Tweets"
                }
            ],
            "source": "!kaggle competitions submit -f submission.csv -m \"Submitting from IBM cloud to Kaggle\" nlp-getting-started"
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "fileName        date                 description                          status    publicScore  privateScore  \r\n--------------  -------------------  -----------------------------------  --------  -----------  ------------  \r\nsubmission.csv  2021-01-30 22:31:42  Submitting from IBM cloud to Kaggle  complete  0.78700      None          \r\nsubmission.csv  2021-01-30 22:30:36  Submitting from IBM cloud to Kaggle  error     None         None          \r\n"
                }
            ],
            "source": "!kaggle competitions submissions nlp-getting-started"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}