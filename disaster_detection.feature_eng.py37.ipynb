{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Data Cleansing and Feature Engineering\n\n## 1. Setting Up Spark Context"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 2. Download data from Object Store"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Please enter value for IBM_API_KEY_ID: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nPlease enter value for IBM_OBJECT_STORE_BUCKET: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
                }
            ],
            "source": "import os\nimport getpass\n\ndef get_or_set_environment_variable(variable):\n    try:\n        var = os.environ[variable]\n    except KeyError:\n        var = getpass.getpass('Please enter value for {:}: '.format(variable))\n    \n    os.environ[variable] = var\n    return var\n\nibm_api_key_id = get_or_set_environment_variable('IBM_API_KEY_ID')\nibm_cloud_store_bucket = get_or_set_environment_variable('IBM_OBJECT_STORE_BUCKET')"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "{'train': ['disaster_detection_train-0000.parquet'],\n 'label': ['disaster_detection_label-0000.parquet'],\n 'test': ['disaster_detection_test-0000.parquet']}"
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "import json\nimport os\n\nimport types\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\nclient = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=ibm_api_key_id,\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client.get_object(Bucket=ibm_cloud_store_bucket,\n                         Key='etl_parquet_files.json')['Body']\n# add missing __iter__ method\n\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\nfiles = json.load(body)\nfiles"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "def load_dataframe(files, **kargs):\n    dfs = []\n    for fn in files:\n        body = client.get_object(Bucket=ibm_cloud_store_bucket,\n                                 Key=fn)['Body']\n        if not hasattr(body, \"__iter__\"):\n            body.__iter__ = types.MethodType( __iter__, body )\n        \n        tfn = 'temp_{:}'.format(fn)\n        with open(tfn, 'wb') as temp:\n            temp.write(body.read())\n        dfs.append(spark.read.options(**kargs).parquet(tfn))\n    df = dfs.pop()\n    for other in dfs:\n        df = df.union(other)\n    return df\n\ndf_train = load_dataframe(files['train'])\ndf_test = load_dataframe(files['test'])\ndf_label = load_dataframe(files['label'])"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "True"
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df_train.schema == df_test.schema"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 2. Data Cleansing"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8</td>\n      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>#flood #disaster Heavy rain causes flash flood...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>13</td>\n      <td>I'm on top of the hill and I can see a fire in...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>14</td>\n      <td>There's an emergency evacuation happening now ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>15</td>\n      <td>I'm afraid that the tornado is coming to our a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   id                                               text\n0   1  Our Deeds are the Reason of this #earthquake M...\n1   4             Forest fire near La Ronge Sask. Canada\n2   5  All residents asked to 'shelter in place' are ...\n3   6  13,000 people receive #wildfires evacuation or...\n4   7  Just got sent this photo from Ruby #Alaska as ...\n5   8  #RockyFire Update => California Hwy. 20 closed...\n6  10  #flood #disaster Heavy rain causes flash flood...\n7  13  I'm on top of the hill and I can see a fire in...\n8  14  There's an emergency evacuation happening now ...\n9  15  I'm afraid that the tornado is coming to our a..."
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "def drop_unused_cols(df):\n    return df.drop('location', 'keyword')\n\ndf_train = drop_unused_cols(df_train)\ndf_test = drop_unused_cols(df_test)\n\ndf_train.limit(10).toPandas()"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting unidecode\n  Downloading Unidecode-1.2.0-py2.py3-none-any.whl (241 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 241 kB 13.0 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: unidecode\nSuccessfully installed unidecode-1.2.0\n"
                }
            ],
            "source": "!pip install unidecode"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>our deeds are the reason of this earthquake ma...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>forest fire near la ronge sask canada</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>all residents asked to shelter in place are be...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>people receive wildfires evacuation orders in ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>just got sent this photo from ruby alaska as s...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8</td>\n      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n      <td>rockyfire update  california hwy closed in bot...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>#flood #disaster Heavy rain causes flash flood...</td>\n      <td>flood disaster heavy rain causes flash floodin...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>13</td>\n      <td>I'm on top of the hill and I can see a fire in...</td>\n      <td>im on top of the hill and i can see a fire in ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>14</td>\n      <td>There's an emergency evacuation happening now ...</td>\n      <td>theres an emergency evacuation happening now i...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>15</td>\n      <td>I'm afraid that the tornado is coming to our a...</td>\n      <td>im afraid that the tornado is coming to our area</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   id                                               text  \\\n0   1  Our Deeds are the Reason of this #earthquake M...   \n1   4             Forest fire near La Ronge Sask. Canada   \n2   5  All residents asked to 'shelter in place' are ...   \n3   6  13,000 people receive #wildfires evacuation or...   \n4   7  Just got sent this photo from Ruby #Alaska as ...   \n5   8  #RockyFire Update => California Hwy. 20 closed...   \n6  10  #flood #disaster Heavy rain causes flash flood...   \n7  13  I'm on top of the hill and I can see a fire in...   \n8  14  There's an emergency evacuation happening now ...   \n9  15  I'm afraid that the tornado is coming to our a...   \n\n                                          clean_text  \n0  our deeds are the reason of this earthquake ma...  \n1              forest fire near la ronge sask canada  \n2  all residents asked to shelter in place are be...  \n3  people receive wildfires evacuation orders in ...  \n4  just got sent this photo from ruby alaska as s...  \n5  rockyfire update  california hwy closed in bot...  \n6  flood disaster heavy rain causes flash floodin...  \n7  im on top of the hill and i can see a fire in ...  \n8  theres an emergency evacuation happening now i...  \n9   im afraid that the tornado is coming to our area  "
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "import re\nimport string\n\nfrom unidecode import unidecode\nimport pyspark.sql.functions as sfun\n\n\n\ndef clean_text(text):    \n    @sfun.udf\n    def normalize_alphabet(text):\n        text = unidecode(text)\n        text = text.encode('ascii', errors='ignore').decode('utf-8', errors='ignore')\n        text = text.lower()\n        return text\n    \n    def no_squares(text):\n        return sfun.regexp_replace(text, '\\[.*?\\]', '')\n    \n    def no_links(text):\n        return sfun.regexp_replace(text, 'https?://\\S+|www\\.\\S+', '')\n    \n    def no_angles(text):\n        return sfun.regexp_replace(text, '<.*?>+', '')\n    \n    def no_punctuation(text):\n        return sfun.regexp_replace(text, '[%s]' % re.escape(string.punctuation), '')\n    \n    def no_newline(text):\n        return sfun.regexp_replace(text, '\\n', '')\n    \n    def no_words_with_nums(text):\n        return sfun.regexp_replace(text, '\\w*\\d\\w*\\s?', '')\n    \n    \n    text = normalize_alphabet(text)\n    text = no_squares(text)\n    text = no_links(text)\n    text = no_angles(text)\n    text = no_punctuation(text)\n    text = no_newline(text)\n    text = no_words_with_nums(text)\n    return text\n\ndf_train = df_train.withColumn('clean_text', clean_text('text'))\ndf_test = df_test.withColumn('clean_text', clean_text('text'))\n\ndf_train.limit(10).toPandas()"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>[all, residents, asked, to, shelter, in, place...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>[people, receive, wildfires, evacuation, order...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8</td>\n      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n      <td>[rockyfire, update, california, hwy, closed, i...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>#flood #disaster Heavy rain causes flash flood...</td>\n      <td>[flood, disaster, heavy, rain, causes, flash, ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>13</td>\n      <td>I'm on top of the hill and I can see a fire in...</td>\n      <td>[im, on, top, of, the, hill, and, i, can, see,...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>14</td>\n      <td>There's an emergency evacuation happening now ...</td>\n      <td>[theres, an, emergency, evacuation, happening,...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>15</td>\n      <td>I'm afraid that the tornado is coming to our a...</td>\n      <td>[im, afraid, that, the, tornado, is, coming, t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   id                                               text  \\\n0   1  Our Deeds are the Reason of this #earthquake M...   \n1   4             Forest fire near La Ronge Sask. Canada   \n2   5  All residents asked to 'shelter in place' are ...   \n3   6  13,000 people receive #wildfires evacuation or...   \n4   7  Just got sent this photo from Ruby #Alaska as ...   \n5   8  #RockyFire Update => California Hwy. 20 closed...   \n6  10  #flood #disaster Heavy rain causes flash flood...   \n7  13  I'm on top of the hill and I can see a fire in...   \n8  14  There's an emergency evacuation happening now ...   \n9  15  I'm afraid that the tornado is coming to our a...   \n\n                                          clean_text  \n0  [our, deeds, are, the, reason, of, this, earth...  \n1      [forest, fire, near, la, ronge, sask, canada]  \n2  [all, residents, asked, to, shelter, in, place...  \n3  [people, receive, wildfires, evacuation, order...  \n4  [just, got, sent, this, photo, from, ruby, ala...  \n5  [rockyfire, update, california, hwy, closed, i...  \n6  [flood, disaster, heavy, rain, causes, flash, ...  \n7  [im, on, top, of, the, hill, and, i, can, see,...  \n8  [theres, an, emergency, evacuation, happening,...  \n9  [im, afraid, that, the, tornado, is, coming, t...  "
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df_train = df_train.select('id', 'text', sfun.split('clean_text', '\\s+').alias('clean_text'))\ndf_test = df_test.select('id', 'text', sfun.split('clean_text', '\\s+').alias('clean_text'))\n\ndf_train.limit(10).toPandas()"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "[nltk_data] Downloading package stopwords to /home/wsuser/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n"
                },
                {
                    "data": {
                        "text/plain": "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre']"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "import re\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nnltk.download('stopwords')\nstop = stopwords.words('english') + [\"I'm\", 'via', 'u'] + [word + \"'s\" for word in stopwords.words('english')]\n\nstop = list(map(lambda word: re.sub('[{:}]'.format(re.escape(string.punctuation)), '', word).lower(),\n                stop))\n\nstop[:10]"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>[residents, asked, shelter, place, notified, o...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>[people, receive, wildfires, evacuation, order...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8</td>\n      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n      <td>[rockyfire, update, california, hwy, closed, d...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>#flood #disaster Heavy rain causes flash flood...</td>\n      <td>[flood, disaster, heavy, rain, causes, flash, ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>13</td>\n      <td>I'm on top of the hill and I can see a fire in...</td>\n      <td>[top, hill, see, fire, woods]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>14</td>\n      <td>There's an emergency evacuation happening now ...</td>\n      <td>[emergency, evacuation, happening, building, a...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>15</td>\n      <td>I'm afraid that the tornado is coming to our a...</td>\n      <td>[afraid, tornado, coming, area]</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   id                                               text  \\\n0   1  Our Deeds are the Reason of this #earthquake M...   \n1   4             Forest fire near La Ronge Sask. Canada   \n2   5  All residents asked to 'shelter in place' are ...   \n3   6  13,000 people receive #wildfires evacuation or...   \n4   7  Just got sent this photo from Ruby #Alaska as ...   \n5   8  #RockyFire Update => California Hwy. 20 closed...   \n6  10  #flood #disaster Heavy rain causes flash flood...   \n7  13  I'm on top of the hill and I can see a fire in...   \n8  14  There's an emergency evacuation happening now ...   \n9  15  I'm afraid that the tornado is coming to our a...   \n\n                                          clean_text  \n0  [deeds, reason, earthquake, may, allah, forgiv...  \n1      [forest, fire, near, la, ronge, sask, canada]  \n2  [residents, asked, shelter, place, notified, o...  \n3  [people, receive, wildfires, evacuation, order...  \n4  [got, sent, photo, ruby, alaska, smoke, wildfi...  \n5  [rockyfire, update, california, hwy, closed, d...  \n6  [flood, disaster, heavy, rain, causes, flash, ...  \n7                      [top, hill, see, fire, woods]  \n8  [emergency, evacuation, happening, building, a...  \n9                    [afraid, tornado, coming, area]  "
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "@sfun.udf\ndef remove_stopwords(text):\n    return list(filter(lambda word: not word in stop, text))\n\ndf_train = df_train.select('id', 'text', remove_stopwords('clean_text').alias('clean_text'))\ndf_test = df_test.select('id', 'text', remove_stopwords('clean_text').alias('clean_text'))\n\ndf_train.limit(10).toPandas()"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "[nltk_data] Downloading package wordnet to /home/wsuser/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\n"
                },
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>[resident, ask, shelter, place, notify, office...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>[people, receive, wildfire, evacuation, order,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>[get, send, photo, ruby, alaska, smoke, wildfi...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8</td>\n      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n      <td>[rockyfire, update, california, hwy, close, di...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>#flood #disaster Heavy rain causes flash flood...</td>\n      <td>[flood, disaster, heavy, rain, cause, flash, f...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>13</td>\n      <td>I'm on top of the hill and I can see a fire in...</td>\n      <td>[top, hill, see, fire, wood]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>14</td>\n      <td>There's an emergency evacuation happening now ...</td>\n      <td>[emergency, evacuation, happen, build, across,...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>15</td>\n      <td>I'm afraid that the tornado is coming to our a...</td>\n      <td>[afraid, tornado, come, area]</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   id                                               text  \\\n0   1  Our Deeds are the Reason of this #earthquake M...   \n1   4             Forest fire near La Ronge Sask. Canada   \n2   5  All residents asked to 'shelter in place' are ...   \n3   6  13,000 people receive #wildfires evacuation or...   \n4   7  Just got sent this photo from Ruby #Alaska as ...   \n5   8  #RockyFire Update => California Hwy. 20 closed...   \n6  10  #flood #disaster Heavy rain causes flash flood...   \n7  13  I'm on top of the hill and I can see a fire in...   \n8  14  There's an emergency evacuation happening now ...   \n9  15  I'm afraid that the tornado is coming to our a...   \n\n                                          clean_text  \n0  [deed, reason, earthquake, may, allah, forgive...  \n1      [forest, fire, near, la, ronge, sask, canada]  \n2  [resident, ask, shelter, place, notify, office...  \n3  [people, receive, wildfire, evacuation, order,...  \n4  [get, send, photo, ruby, alaska, smoke, wildfi...  \n5  [rockyfire, update, california, hwy, close, di...  \n6  [flood, disaster, heavy, rain, cause, flash, f...  \n7                       [top, hill, see, fire, wood]  \n8  [emergency, evacuation, happen, build, across,...  \n9                      [afraid, tornado, come, area]  "
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from nltk.stem import WordNetLemmatizer\n\nnltk.download('wordnet')\n\nlemma = WordNetLemmatizer()\n\n@sfun.udf\ndef lemmatize_words(words):\n    def exhaustive_lemmatization(word):\n        for pos in ['n', 'v', 'a']:\n            lem = lemma.lemmatize(word, pos=pos)\n            if lem != word:\n                return lem\n        return word\n    return list(map(exhaustive_lemmatization, words))\n\ndf_train = df_train.select('id', 'text', lemmatize_words('clean_text').alias('clean_text'))\ndf_test = df_test.select('id', 'text', lemmatize_words('clean_text').alias('clean_text'))\n\ndf_train.limit(10).toPandas()"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>deed reason earthquake may allah forgive u</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>forest fire near la ronge sask canada</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>resident ask shelter place notify officer evac...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>people receive wildfire evacuation order calif...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>get send photo ruby alaska smoke wildfire pour...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8</td>\n      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n      <td>rockyfire update california hwy close directio...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>#flood #disaster Heavy rain causes flash flood...</td>\n      <td>flood disaster heavy rain cause flash flood st...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>13</td>\n      <td>I'm on top of the hill and I can see a fire in...</td>\n      <td>top hill see fire wood</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>14</td>\n      <td>There's an emergency evacuation happening now ...</td>\n      <td>emergency evacuation happen build across street</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>15</td>\n      <td>I'm afraid that the tornado is coming to our a...</td>\n      <td>afraid tornado come area</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   id                                               text  \\\n0   1  Our Deeds are the Reason of this #earthquake M...   \n1   4             Forest fire near La Ronge Sask. Canada   \n2   5  All residents asked to 'shelter in place' are ...   \n3   6  13,000 people receive #wildfires evacuation or...   \n4   7  Just got sent this photo from Ruby #Alaska as ...   \n5   8  #RockyFire Update => California Hwy. 20 closed...   \n6  10  #flood #disaster Heavy rain causes flash flood...   \n7  13  I'm on top of the hill and I can see a fire in...   \n8  14  There's an emergency evacuation happening now ...   \n9  15  I'm afraid that the tornado is coming to our a...   \n\n                                          clean_text  \n0         deed reason earthquake may allah forgive u  \n1              forest fire near la ronge sask canada  \n2  resident ask shelter place notify officer evac...  \n3  people receive wildfire evacuation order calif...  \n4  get send photo ruby alaska smoke wildfire pour...  \n5  rockyfire update california hwy close directio...  \n6  flood disaster heavy rain cause flash flood st...  \n7                             top hill see fire wood  \n8    emergency evacuation happen build across street  \n9                           afraid tornado come area  "
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "@sfun.udf\ndef join_words(words):\n    return ' '.join(words)\n\ndf_train = df_train.select('id', 'text', join_words('clean_text').alias('clean_text'))\ndf_test = df_test.select('id', 'text', join_words('clean_text').alias('clean_text'))\n\ndf_train.limit(10).toPandas()"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "14101\n"
                },
                {
                    "data": {
                        "text/plain": "[('get', (118, 310)),\n ('like', (99, 292)),\n ('fire', (266, 89)),\n ('amp', (106, 192)),\n ('go', (81, 194)),\n ('new', (57, 172)),\n ('bomb', (171, 50)),\n ('one', (69, 134)),\n ('people', (106, 93)),\n ('say', (93, 105)),\n ('news', (132, 53)),\n ('burn', (86, 94)),\n ('kill', (156, 17)),\n ('make', (42, 129)),\n ('video', (71, 97)),\n ('flood', (120, 45)),\n ('u', (81, 81)),\n ('time', (58, 104)),\n ('crash', (110, 51)),\n ('come', (52, 108))]"
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df = df_train.join(df_label, on='id', how='inner')\n\nwords = df.rdd.flatMap(\n    lambda row: [(word, (int(row['target']), 1 - int(row['target']))) for word in row['clean_text'].split()])\nwordcount = words.reduceByKey(lambda agg, cat: (agg[0] + cat[0], agg[1] + cat[1]))\nprint(wordcount.count())\ntop20_words = wordcount.sortBy(keyfunc=lambda row: -sum(row[1])).take(20)\ntop20_words"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": "import plotly.io as pio\npio.renderers.default = 'notebook_connected'"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": "<div>\n        \n        \n            <div id=\"fb852d9f-9318-4d53-9ac1-739dc7618016\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n            <script type=\"text/javascript\">\n                require([\"plotly\"], function(Plotly) {\n                    window.PLOTLYENV=window.PLOTLYENV || {};\n                    \n                if (document.getElementById(\"fb852d9f-9318-4d53-9ac1-739dc7618016\")) {\n                    Plotly.newPlot(\n                        'fb852d9f-9318-4d53-9ac1-739dc7618016',\n                        [{\"alignmentgroup\": \"True\", \"hovertemplate\": \"target=0<br>word=%{x}<br>count=%{text}<extra></extra>\", \"legendgroup\": \"0\", \"marker\": {\"color\": \"#636efa\"}, \"name\": \"0\", \"offsetgroup\": \"0\", \"orientation\": \"v\", \"showlegend\": true, \"text\": [310.0, 292.0, 89.0, 192.0, 194.0, 172.0, 50.0, 134.0, 93.0, 105.0, 53.0, 94.0, 17.0, 129.0, 97.0, 45.0, 81.0, 104.0, 51.0, 108.0], \"textposition\": \"auto\", \"type\": \"bar\", \"x\": [\"get\", \"like\", \"fire\", \"amp\", \"go\", \"new\", \"bomb\", \"one\", \"people\", \"say\", \"news\", \"burn\", \"kill\", \"make\", \"video\", \"flood\", \"u\", \"time\", \"crash\", \"come\"], \"xaxis\": \"x\", \"y\": [310, 292, 89, 192, 194, 172, 50, 134, 93, 105, 53, 94, 17, 129, 97, 45, 81, 104, 51, 108], \"yaxis\": \"y\"}, {\"alignmentgroup\": \"True\", \"hovertemplate\": \"target=1<br>word=%{x}<br>count=%{text}<extra></extra>\", \"legendgroup\": \"1\", \"marker\": {\"color\": \"#EF553B\"}, \"name\": \"1\", \"offsetgroup\": \"1\", \"orientation\": \"v\", \"showlegend\": true, \"text\": [118.0, 99.0, 266.0, 106.0, 81.0, 57.0, 171.0, 69.0, 106.0, 93.0, 132.0, 86.0, 156.0, 42.0, 71.0, 120.0, 81.0, 58.0, 110.0, 52.0], \"textposition\": \"auto\", \"type\": \"bar\", \"x\": [\"get\", \"like\", \"fire\", \"amp\", \"go\", \"new\", \"bomb\", \"one\", \"people\", \"say\", \"news\", \"burn\", \"kill\", \"make\", \"video\", \"flood\", \"u\", \"time\", \"crash\", \"come\"], \"xaxis\": \"x\", \"y\": [118, 99, 266, 106, 81, 57, 171, 69, 106, 93, 132, 86, 156, 42, 71, 120, 81, 58, 110, 52], \"yaxis\": \"y\"}],\n                        {\"barmode\": \"relative\", \"legend\": {\"title\": {\"text\": \"target\"}, \"tracegroupgap\": 0}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Top 20 Words per Target\"}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"word\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"count\"}}},\n                        {\"responsive\": true}\n                    ).then(function(){\n                            \nvar gd = document.getElementById('fb852d9f-9318-4d53-9ac1-739dc7618016');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })\n                };\n                });\n            </script>\n        </div>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": "import pandas as pd\nimport plotly.express as px\n\nc_words, counts = zip(*top20_words)\nc_words *= 2\n\ncount1, count0 = zip(*counts)\nc_counts = count0 + count1\n\nc_target = ['0'] * len(counts) + ['1'] * len(counts)\n\nfig = px.bar(pd.DataFrame({'word': c_words,\n                     'count': c_counts,\n                     'target': c_target\n                    }),\n             x='word',\n             y='count',\n             text='count',\n             title='Top 20 Words per Target',\n             color='target')\n\nfig.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 3. Feature engineering\n\nWe create 3 features from the data and will evaluate performance.\n\n* `CountVectorizer`: Converts a text document to a sparse vector of token counts.\n* `TF-IDF`: Short for \u2018term frequency\u2013inverse document frequency\u2019, is a numerical statistic that is intended to reflect how important a word is. In short we count the occurences of the word per document and normalize the count by taking the overall frequency of the term into account.\n* `Word2Vec`: A neural network model is trained to learn word associations from a large corpus of text."
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+--------------------+--------------------+--------------------+--------------------+\n| id|                text|      features_count|      features_tfidf|        features_w2v|\n+---+--------------------+--------------------+--------------------+--------------------+\n|  1|Our Deeds are the...|(2266,[16,80,201,...|(2500,[26,166,336...|[-0.0027080011953...|\n|  4|Forest fire near ...|(2266,[2,139,183,...|(2500,[191,974,20...|[-0.0017992729055...|\n|  5|All residents ask...|(2266,[212,318,32...|(2500,[294,691,11...|[0.00301898798947...|\n|  6|13,000 people rec...|(2266,[8,41,85,21...|(2500,[8,325,644,...|[-0.0085309529677...|\n|  7|Just got sent thi...|(2266,[0,85,130,1...|(2500,[178,198,32...|[-0.0092202152849...|\n|  8|#RockyFire Update...|(2266,[2,41,85,21...|(2500,[191,325,64...|[-0.0043081726810...|\n| 10|#flood #disaster ...|(2266,[15,21,57,1...|(2500,[353,356,64...|[0.00218058816002...|\n| 13|I'm on top of the...|(2266,[2,25,187,1...|(2500,[15,191,187...|[-0.0103795815724...|\n| 14|There's an emerge...|(2266,[20,198,212...|(2500,[36,99,699,...|[0.00826060997011...|\n| 15|I'm afraid that t...|(2266,[19,239,322...|(2500,[649,1361,1...|[0.00754532904829...|\n| 16|Three people died...|(2266,[8,109,166,...|(2500,[229,236,92...|[0.01370342611335...|\n| 17|Haha South Tampa ...|(2266,[0,15,104,2...|(2500,[5,52,263,3...|[0.00281095398434...|\n| 18|#raining #floodin...|(2266,[15,29,173,...|(2500,[52,605,113...|[0.00221913085422...|\n| 19|#Flood in Bago My...|(2266,[15,731,774...|(2500,[1143,1277,...|[0.00140541605651...|\n| 20|Damage to school ...|(2266,[18,52,93,1...|(2500,[823,949,13...|[0.00613656788066...|\n| 23|      What's up man?|   (2266,[53],[1.0])|(2500,[730],[4.21...|[0.01679854281246...|\n| 24|       I love fruits|(2266,[37,1534],[...|(2500,[1240,2048]...|[0.00770199950784...|\n| 25|    Summer is lovely|(2266,[272,1477],...|(2500,[1933,2228]...|[0.01030138239730...|\n| 26|   My car is so fast|(2266,[52,515],[1...|(2500,[1763,2372]...|[0.00533527706284...|\n| 28|What a goooooooaa...|        (2266,[],[])| (2500,[2136],[0.0])|[0.0,0.0,0.0,0.0,...|\n+---+--------------------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, HashingTF, IDF, Word2Vec\nfrom pyspark.ml import Pipeline\n\nregex_tokenizer = RegexTokenizer(inputCol='clean_text', outputCol='words', pattern=r'\\W')\ncounter = CountVectorizer(inputCol='words', outputCol='features_count', vocabSize=2500, minDF=5)\nhashing_tf = HashingTF(inputCol='words', outputCol='raw_features', numFeatures=2500)\nidf = IDF(inputCol='raw_features', outputCol='features_tfidf', minDocFreq=5)\nword2vec = Word2Vec(inputCol='words', outputCol='features_w2v', maxSentenceLength=50)\n\npipeline = Pipeline(stages=[regex_tokenizer,\n                            counter,\n                            hashing_tf,\n                            idf,\n                            word2vec]).fit(df_train)\n\ndf_eng_train = pipeline.transform(df_train).select('id', 'text', 'features_count', 'features_tfidf', 'features_w2v')\ndf_eng_test = pipeline.transform(df_test).select('id', 'text', 'features_count', 'features_tfidf', 'features_w2v')\n\ndf_eng_train.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 4. Serializing the dataframes in *Parquet* format"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": "!rm -r ./disaster_detection_*"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "['./disaster_detection_clean_test', './disaster_detection_clean_train']"
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "import glob\n\ntemp_parquet_file = os.path.join(os.path.curdir,\n                                 'disaster_detection_clean_{}')\ndf_eng_train.write.parquet(temp_parquet_file.format('train'), mode='overwrite')\ndf_eng_test.write.parquet(temp_parquet_file.format('test'), mode='overwrite')\n\nglob.glob(temp_parquet_file.format('*'))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 5. Uploading the files to object cloud"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "{'train': ['disaster_detection_clean_train-0000.parquet'], 'test': ['disaster_detection_clean_test-0000.parquet']}\n"
                }
            ],
            "source": "def upload_parquet(client, path):\n    parts = glob.glob(os.path.join(path, '*.parquet'))\n    parquets = ['{:s}-{:04d}.parquet'.format(os.path.split(path)[-1], i)\n                for i in range(len(parts))]\n    for part, parquet in zip(parts, parquets):\n        with open(part, 'rb') as parquetF:\n            client.put_object(Bucket=ibm_cloud_store_bucket,\n                          Body=parquetF,\n                          Key=parquet\n                         )\n    return parquets\n\nclient = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=ibm_api_key_id,\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\n\nparquets = {}\nfor dataset in ('train', 'test'):\n    parquets[dataset] = upload_parquet(client, temp_parquet_file.format(dataset))\n\nprint(parquets)"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "{'ResponseMetadata': {'RequestId': 'e7b44b6f-46d0-49b1-9ae1-0c04548a2065',\n  'HostId': '',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'date': 'Sat, 06 Feb 2021 21:46:11 GMT',\n   'x-clv-request-id': 'e7b44b6f-46d0-49b1-9ae1-0c04548a2065',\n   'server': 'Cleversafe',\n   'x-clv-s3-version': '2.5',\n   'x-amz-request-id': 'e7b44b6f-46d0-49b1-9ae1-0c04548a2065',\n   'etag': '\"fa8e4e3acd022a663841a2bb8fc8265c\"',\n   'content-length': '0'},\n  'RetryAttempts': 0},\n 'ETag': '\"fa8e4e3acd022a663841a2bb8fc8265c\"'}"
                    },
                    "execution_count": 22,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "import json\n\n\nparquets['label'] = files['label']\nclient.put_object(Bucket=ibm_cloud_store_bucket,\n                  Body=json.dumps(parquets),\n                  Key='feature_eng_parquet_files.json')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}